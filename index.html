<!DOCTYPE HTML>
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Apple Privacy Letter: An Open Letter Against Apple's Privacy-Invasive Content Scanning Technology</title>
	<meta name="description" content="Read and sign the open letter protesting against Apple's roll-out of new content-scanning technology that threatens to overturn individual privacy on a global scale, and to reverse progress achieved with end-to-end encryption for all.">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link href="res/css/style.css" rel="stylesheet" type="text/css" media="all">
	<script src="res/js/script.js" charset="utf-8"></script>
	<meta name="twitter:card" content="summary_large_image" />
	<meta name="twitter:site" content="@kaepora" />
	<meta name="twitter:title" content="Apple Privacy Letter" />
	<meta name="twitter:description" content="Read and sign the open letter protesting against Apple's roll-out of new content-scanning technology that threatens to overturn individual privacy on a global scale, and to reverse progress achieved with end-to-end encryption for all." />
	<meta name="twitter:url" content="https://appleprivacyletter.com" />
	<meta name="twitter:image" content="https://appleprivacyletter.com/res/img/appleprivacyletter_logo_white.png" />
	<meta name="twitter:creator" content="@kaepora" />
	<link rel="icon" type="image/png" href="https://appleprivacyletter.com/res/img/favicon.png" />
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Georama:wght@300;400&family=Open+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=STIX+Two+Text:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
</head>

<body>
	<div id="main">
		<div id="heading">
			<img src="res/img/appleprivacyletter_logo_transparent.png" alt="" id="logo" />
			<h1>An Open Letter Against Apple's Privacy-Invasive Content Scanning Technology</h1>
			<h2>Security &amp; Privacy Experts, Cryptographers, Researchers, Professors, Legal Experts and Apple Consumers Decry Apple's Planned Move to Undermine User Privacy and End-to-End Encryption</h2>
		</div>
		<div id="letter">
			<h2>Dear Apple,</h2>
			<p>
				On August 5th, 2021, Apple Inc. <a href="https://www.apple.com/child-safety/" target="_blank">announced</a> new technological measures meant to apply across virtually all of its devices under the umbrella of <em>&ldquo;Expanded Protections for Children&rdquo;</em>. While child exploitation is a serious problem, and while efforts to combat it are almost unquestionably well-intentioned, <strong>Apple's proposal introduces a backdoor that threatens to undermine fundamental privacy protections for all users of Apple products.</strong>
			</p>
			<p>
				Immediately after Apple's announcement, experts around the world sounded the alarm on how Apple's proposed measures could turn every iPhone into a device that is continuously scanning all photos and messages that pass through it in order to report any objectionable content to law enforcement, setting a precedent where our personal devices become a radical new tool for invasive surveillance, with little oversight to prevent eventual abuse and unreasonable expansion of the scope of surveillance.
			</p>
			<p>
				The <a href="https://www.eff.org/deeplinks/2021/08/apples-plan-think-different-about-encryption-opens-backdoor-your-private-life" target="_blank">Electronic Frontier Foundation has said</a> that <strong><em>&ldquo;Apple is opening the door to broader abuses&rdquo;</em></strong>:
			</p>
			<blockquote>
				&ldquo;It’s impossible to build a client-side scanning system that can only be used for sexually explicit images sent or received by children. As a consequence, even a well-intentioned effort to build such a system will break key promises of the messenger’s encryption itself and open the door to broader abuses [&mldr;] That’s not a slippery slope; that’s a fully built system just waiting for external pressure to make the slightest change.&rdquo;
			</blockquote>
			<p>
				The <a href="" target="_blank">Center for Democracy and Technology</a> has said that it is <strong><em>&ldquo;deeply concerned that Apple’s changes in fact create new risks to children and all users, and mark a significant departure from long-held privacy and security protocols&rdquo;</em></strong>:
			</p>
			<blockquote>
				&ldquo;Apple is replacing its industry-standard end-to-end encrypted messaging system with an infrastructure for surveillance and censorship, which will be vulnerable to abuse and scope-creep not only in the U.S., but around the world,&rdquo; says Greg Nojeim, Co-Director of CDT’s Security &amp; Surveillance Project. &ldquo;Apple should abandon these changes and restore its users’ faith in the security and integrity of their data on Apple devices and services.&rdquo;
			</blockquote>
			<p>
				Dr. Carmela Troncoso, a leading research expert in Security &amp; Privacy and professor at EPFL in Lausanne, Switzerland, <a href="https://twitter.com/carmelatroncoso/status/1423554795487518723" target="_blank">has said</a> that while <strong><em>&ldquo;Apple's new detector for child sexual abuse material (CSAM) is promoted under the umbrella of child protection and privacy, it is a firm step towards prevalent surveillance and control&rdquo;</em></strong>.
			</p>
			<p>
				Dr. Matthew D. Green, another leading research expert in Security &amp; Privacy and professor at the Johns Hopkins University in Baltimore, Maryland, <a href="https://twitter.com/matthew_d_green" target="_blank">has said</a> that <strong><em>&ldquo;yesterday we were gradually headed towards a future where less and less of our information had to be under the control and review of anyone but ourselves. For the first time since the 1990s we were taking our privacy back. Today we’re on a different path.&rdquo;</em></strong>, <a href="https://www.wired.com/story/apple-csam-detection-icloud-photos-encryption-privacy/" target="_blank">adding</a>:
			</p>
			<blockquote>
				&ldquo;The pressure is going to come from the UK, from the US, from India, from China. I'm terrified about what that's going to look like. Why Apple would want to tell the world, ‘Hey, we've got this tool’?&rdquo;
			</blockquote>
			<p>
				Dr. Nadim Kobeissi, a researcher in Security &amp; Privacy issues, <a href="https://twitter.com/kaepora/status/1423388549529968645">warned</a>:
			</p>
			<blockquote>
				&rdquo;Apple sells iPhones without FaceTime in Saudi Arabia, because local regulation prohibits encrypted phone calls. That's just one example of many where Apple's bent to local pressure. What happens when local regulations in Saudi Arabia mandate that messages be scanned not for child sexual abuse, but for homosexuality or for offenses against the monarchy?&rdquo;
			</blockquote>
			<p>
				The type of technology that Apple is proposing for its child protection measures depends on an expandable infrastructure that can't be monitored or technically limited. Experts have <a href="https://twitter.com/carmelatroncoso/status/1423554811086188548" target="_blank">repeatedly warned</a> that the problem isn't just privacy, but also the lack of accountability, technical barriers to expansion, and lack of analysis or even acknowledgement of the potential for errors and false positives.
 			</p>
			 <p>
				 Kendra Albert, a lawyer at the Harvard Law School's Cyberlaw Clinic, <a href="https://twitter.com/KendraSerra/status/1423365222841135114" target="_blank">has warned</a> that <strong><em>&ldquo;these "child protection" features are going to get queer kids kicked out of their homes, beaten, or worse.&rdquo;</em></strong>, adding:
			 </p>
			 <blockquote>
				&ldquo;I just know (calling it now) that these machine learning algorithms are going to flag transition photos. Good luck texting your friends a picture of you if you have "female presenting nipples."&rdquo;
			 </blockquote>
			 <h3>Our Request</h3>
			 <p>
				 We, the undersigned, ask that:
			 </p>
			 <ol>
				 <li>Apple Inc.'s' deployment of its proposed content monitoring technology is halted immediately.</li>
				 <li>Apple Inc. issue a statement reaffirming their commitment to end-to-end encryption and to user privacy.</li>
			 </ol>
			 <p>
				 Apple's current path threatens to undermine decades of work by technologists, academics and policy advocates towards strong privacy-preserving measures being the norm across a majority of consumer electronic devices and use cases. We ask that Apple reconsider its technology rollout, lest it undo that important work.
			 </p>
		</div>
		<div id="signatures">
			<h2>Signatures</h2>
			<h3>Organizations</h3>
			<ul>
				<li><em><a href="https://github.com/nadimkobeissi/appleprivacyletter/pulls">Please add your signature by submitting a pull request.</a></em></li>
			</ul>
			<h3>Individuals</h3>
			<ol>
				<li><em><a href="https://github.com/nadimkobeissi/appleprivacyletter/pulls">Please add your signature by submitting a pull request.</a></em></li>
				<li>Nadim Kobeissi</li>
			</ol>
		</div>
		<div id="footer">
			<p>
				Letter written August 6th, 2021.
			</p>
		</div>
	</div>
</body>

</html>